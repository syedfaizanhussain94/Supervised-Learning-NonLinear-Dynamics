\section{NMF speech source separation}
\label{nmfsec}

%
We consider the setting in which we are given a temporal signal $x(t)$ that is the sum of  
two speech signals $x_i(t)$, $i=1,2$:
\begin{equation}
\label{ssep}
x(t) = x_1(t) + x_2(t)~,
\end{equation}
and we aim at finding estimates $\widehat{x_i}(t)$.
NMF-based source separation 
techniques typically operate on a non-negative time-frequency representation of $x(t)$, 
such as the spectrogram or the power spectrum,
that we denote as $\Phi(x) \in \RR^{m \times n}$, comprising $m$ frequency bins and $n$ temporal frames. 
NMF attempts to find the non-negative activations $Z_i \in \RR^{q \times n}$, $i=1,2$ 
best representing the different speech components in two dictionaries ${D}_{i} \in \RR^{m \times q}$.
%
This task is achieved through the solution of %the minimization problem
\begin{eqnarray}
\label{eq:optim_general}
\min_{ Z_i \ge 0 } D( \Phi(x) |  \, \sum_{i=1,2} {D}_i Z_i  ) + 
\lambda\, \sum_{i=1,2} \mathcal{R}(Z_i)~.
\end{eqnarray}
The first term in the optimization objective measures the dissimilarity between the input data and the estimated channels. 
%Typically, this data fitting term is assumed to be separable,  
%$$
%D( \bb{A} | \bb{B} ) = \sum_{i,j} D(a_{ij} | b_{ij}).
%$$ 
Frequent choices of $\mu$ are the squared Euclidean distance,
the Kullback-Leibler divergence, and the Itakura-Saito divergence, for which there exist standard optimization algorithms \cite{fevotte2011algorithms}.
In this work we concentrate on a reweighted Euclidean distance, but any other option could be used instead.
%
%Significant attention has been devoted in the literature to the case in which the scalar divergence $D$
%belongs to the family of the so-called $\beta$-divergences, 
%\begin{equation*}
%D_{\beta} (a|b) =\!\!\left \{
%\begin{array}{ll}
%\!\frac{a}{b} - \log{\frac{a}{b}}-1 & \! : \,\beta = 0,\\
%\! a \log{a/b} + (a-b) &\! : \, \beta = 1,\\
%\!\frac{1}{\beta(\beta-1)} (a^{\beta} +(\beta-1)b^{\beta} -\beta a b^{\beta-1}) &\! : \textrm{otherwise.}\\
%\end{array}
%\right.
%\end{equation*}
%This family includes 
%where the three most widely used cost functions in NMF: the squared Euclidean distance ($\beta=2$),
%the Kullback-Leibler divergence ($\beta=1$), and the Itakura-Saito divergence ($\beta=0$).
%For $\beta \ge 1$, the divergence is convex. The case of $\beta=0$ is attractive despite the lack of convexity, due to the scale-invariance of the Itakura-Saito divergence, which makes the NMF procedure insensitive to volume changes
%
The second term in the minimization objective is included to promote some desired structure of the activations. 
Once the optimal activations are solved for, the spectral envelopes of the speech and the noise are 
estimated as $ {D}_{i} Z_{i}$. Since these estimated speech spectrum envelope contain no phase information, 
they are used to build soft masks to filter the mixture signal \cite{schmidt07mlsp}.
%This is done using a designed regularization function $\mathcal{R}$ 
%and its relative importance is controlled by the parameters $\lambda$. 
%Since these estimated speech spectrum envelope contains no phase information, speech signal is estimated
%from the mixture by Wiener filtering. 


%In supervised NMF, the speech and noise dictionaries are trained independently from available training data. 
%
%The underlying assumption of this approach is that the speech and the noise signals forming the mixture are sufficiently distinct to be unambiguously decomposed 
%into $\bb{V} \approx  \bb{W}_{\mathrm{s}} \bb{H}_{\mathrm{s}} + \bb{W}_{\mathrm{n}}\bb{W}_{\mathrm{n}}$. 
%However, this assumption is often violated, e.g., in the presence of multitalker babble noise, when the learned speech and noise dictionaries might be very similar (or coherent).
%
%In other words, the independently trained dictionaries do not ensure that the solutions $\bb{W}_{\mathrm{s}} \bb{H}_{\mathrm{s}}$ and $\bb{W}_{\mathrm{n}} \bb{H}_{\mathrm{n}}$ obtained from \eqref{eq:optim} will resemble the original components of the mixture. 
