\section{Introduction}

The problem of source separation has been widely
studied in the speech processing community \cite{loizou2007speech,hansler2008speech}. 
It becomes particularly challenging when only one microphone is used, or in the presence of 
non-stationary background noise, which is a very common situation in many applications encountered, e.g., in telephony.
We approach this problem as a monaural source separation method
by modeling the speech at an appropriate temporal resolution.
%This is a natural approach when the characteristics of both the signal of interest and the noise vary throughout time \cite{WilsonRSD08,JoderWEVS12,MysoreS11,DuanMS12}.
%Consequently, many works have used source separation techniques for addressing this problem,

The decomposition of time-frequency representations, such as the power or magnitude spectrogram
in terms of elementary atoms of a dictionary, has become a popular tool in audio processing. 
Non-negative matrix factorization (NMF) \cite{NMF},
have been widely adopted in various audio processing tasks, including in particular source separation, see \cite{smaragdis2014static} for a recent review. 
There are many works that follow this line in speech separation \cite{schmidt06speechseparation,shashanka_icassp07} and enhancement \cite{DuanMS12,mohammadiha2013supervised}. %and robust automatic speech recognition \cite{GemmekeVH11,WeningerWGSGHVR12},
%among many others.
%bandwidth extension \cite{BansalRS05,HanMP12} and speaker recognition \cite{wu2010robust,joder2012exploring}.

%NMF and PLCA produce high quality separation results when the dictionaries
%for different sources are sufficiently distinct.
%There is naturally a compromise between the approximation
%of the training data and tightness of the model: the more general is the dictionary the higher is the chance it will include elements that
%match spectral patterns in the competing sources.
%In order to mitigate this problem, recent approaches have proposed
%alternative models constraining the solution in meaningful ways,
%as for example, by imposing sparsity of the activations \cite{shashanka_icassp07,hoyer2004non}.
%

In NMF, signals that can be well approximated with the learned dictionary are
likely to resemble the training data on a frame by frame manner. They might, however, 
not be temporally consistent at larger temporal scales.
 Standard NMF approaches treat different time-frames independently, ignoring the 
temporal dynamics of the signals. 
%In other words, there is additional structure in speech at a time-scale larger than
%the frame-length that cannot be learned (or exploited) with NMF. 
To capture temporal dependencies,  
works have consider convolutional extension of NMF \cite{smaragdis2007convolutive}, where the atoms
correspond to patches comprised of several time-frequency frames. 
%and achieve consistency at a larger temporal scale,
To avoid the increase in dimensionality, many works have proposed regularized extensions of NMF to promote learned structure in the codes. 
Examples of these approaches are, temporal smoothness of the activation coefficients \cite{fevotte2011majorization}, including  co-occurrence statistics of the basis functions \cite{WilsonRSD08}, and learned temporal dynamics \cite{MysoreS11,HanMP12,icassp13a}.
%Recent studies have proposed regularized variants of NMF or PLCA trying to overcome this limitation,

NMF-based source separation methods can be thought as the concatenation of two operators.
First, the signal is represented in a feature space given by a non-linear analysis operator, 
typically defined as the magnitude of a time-frequency representation such
as the Short-Time Fourier Transform (STFT). 
Then a synthesis operator, given by the dictionary learning stage, is applied to produce an unmixing in the feature space.
 The separation is obtained by inverting these representations. 
Performing the separation in the non-linear representation is key to the success of the algorithm. The magnitude STFT is in general sparse (simplifying the separation process) and invariant to variations in the phase, thus relieving the NMF from
learning this irrelevant variability. 
This comes at the expense of inverting the unmixed estimates in the feature space, normally known as the
phase recovery problem \cite{yonina}. In the case of standard NMF, this is 
typically done via Wiener filtering. 

In this work, rather than optimizing a coding scheme with improved temporal coherence, 
we concentrate in the extracton of discriminative and stable features. 
For that purpose, it is crucial to increase the temporal context of the representation, 
reducing uninformative variability while preserving distinctive speech characteristics.
Increasing the temporal scale of STFT or MEL representations results 
in loss of important discriminative information \cite{deepscatt}. 
In order to overcome the limitations of these shallow representations, 
scattering transforms \cite{deepscatt, pami} cascade several 
stages of complex wavelet decompositions and complex modulus, yielding 
discriminative representations with the ability to capture temporal structures at larger scales, 
e.g. smooth changes in pitch and envelope. 
Scattering transforms achieve state-of-the-art results on auditory texture discrimination, and music genre recognition \cite{deepscatt, phdjoan}.
Recently they have been considered in the setting of blind source separation in \cite{stephane_bss}, 
but here we concentrate in the supervised (and semi-supervised) framework.

In source separation, it is desirable to have a representation with better spatio-temporal resolution than standard scattering features. 
In this work, we propose a scattering pyramid representation, consisting in a collection of scattering features at different temporal resolutions.
With this representation, singals are modeled using a multi-level set of dictionaries each acting at a given temporal resolution. 
In this way, short-term temporal dynamics of the signal can be captured by the long-context model, capitalizing on the stability properties of scattering coefficients \cite{pami}.

%Scattering transforms have recently been introduced \cite{XX} to represent audio signals and images, A scattering transform iterates on complex wavelet transforms and modulus operators which compute their envelop. It has close relations with psychophysical and physiological models \cite{XX}. %Their success in audio classification comes from the fact that they can reduce the intra class variability (like temporal redundancy).

Our claim is that an important part of the consistency that is imposed via structured NMF, can be eliminated with a better signal representation.
In this new setting one can learn the temporal dynamics with a very simple NMF encoding. 
However, the problem that becomes
more difficult is that of inverting the non-linear representation. 
Recent studies in textured sound synthesis from scattering
moments have solved this problem successfully using gradient descent algorithms \cite{bruna2013audio}.
Sparse synthesis models with coherent dictionaries are known to be highly unstable representations \cite{jenatton2012local}. 
Thus, training them to satisfy slowness and temporal consistency can be challenging. 
%For example, in \cite{icassp13a}
%the authors explain that the learning the temporal dynamics in NMF via a Kalman type of model, can become very difficult
%when the coding is sparse due to the instability and jitter in the codes.
In contrast, analysis operators are stable by construction.

Recent works have started to use discriminative training in the context of source separation.
Methods based on NMF \cite{weninger2014discriminative,sprechmann2014supervised} and recurrent neural networks \cite{merl,Huang_DNN_Separation_ICASSP2014}.
These initial results show the benefits of this setting. The proposed method could also be framed into
the discriminative setting.

Section \ref{nmfsec} reviews non-negative matrix factorization, while Section \ref{scattsec} describes scattering 
representations for speech. Our source separation algorithm is described in Section \ref{algosec} and 
numerical experiments on TIMIT and GRID datasets are reported in Section \ref{sec:experiments}.

%In all these methods the audio signal is represented as a non-negative and the demixng is obtained  model is expressed as the minimization of a cost with a data fitting term and some structure-promoting penalties.



